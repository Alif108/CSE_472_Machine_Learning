{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(329, 20530)\n",
      "(329,)\n",
      "(83, 20530)\n",
      "(83,)\n"
     ]
    }
   ],
   "source": [
    "X_train = pickle.load(open('../Data/X_train.pkl', 'rb'))\n",
    "y_train = pickle.load(open('../Data/y_train.pkl', 'rb'))\n",
    "X_test = pickle.load(open('../Data/X_test.pkl', 'rb'))\n",
    "y_test = pickle.load(open('../Data/y_test.pkl', 'rb'))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.48855657  0.12112708 -0.33372349 ...  0.04735987  0.50779996\n",
      "  -0.02217494]\n",
      " [ 0.83092164  1.17948768 -0.33372349 ...  0.18754437  0.77965228\n",
      "  -0.17205193]\n",
      " [-1.861054   -0.69352094 -0.33372349 ... -0.35346398 -2.00005787\n",
      "   3.63814136]\n",
      " ...\n",
      " [ 1.50287947  0.88173571  0.1048791  ... -0.33180584  0.91213697\n",
      "  -0.14253948]\n",
      " [-0.06583194  0.60359346 -0.33372349 ...  0.20030191  0.44110666\n",
      "  -1.31512921]\n",
      " [-0.87784052 -0.01478846 -0.33372349 ...  0.12805867  1.46177518\n",
      "  -0.6430939 ]]\n"
     ]
    }
   ],
   "source": [
    "# standardize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([329, 20530]),\n",
       " torch.Size([83, 20530]),\n",
       " torch.Size([329]),\n",
       " torch.Size([83]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   4   6   8  10  11  12  13  14  15  17  18  19  20  21  22\n",
      "  23  24  26  27  28  29  30  31  32  34  35  36  37  38  39  40  41  43\n",
      "  44  47  48  49  50  51  52  53  54  56  58  59  61  62  64  65  66  67\n",
      "  68  69  70  71  72  74  75  76  79  80  81  83  85  86  87  88  89  91\n",
      "  92  93  95  96  97  98  99 100 102 103 104 105 106 107 108 109 112 113\n",
      " 114 115 116 117 118 120 121 122 123 125 126 127 128 129 130 131 132 133\n",
      " 134 135 136 138 141 142 143 146 148 149 150 151 152 153 154 155 156 157\n",
      " 158 159 160 161 162 165 166 167 169 170 171 174 178 179 180 181 183 184\n",
      " 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202\n",
      " 203 204 205 206 207 208 209 211 212 213 214 215 216 217 218 220 221 223\n",
      " 224 225 226 227 228 229 230 231 233 234 235 236 238 239 240 241 242 243\n",
      " 244 245 246 247 248 249 251 252 253 254 255 257 258 259 261 263 264 265\n",
      " 266 267 268 269 270 272 273 276 278 279 280 281 282 283 284 285 286 287\n",
      " 290 291 292 293 294 295 298 299 300 302 303 304 306 307 308 310 311 313\n",
      " 316 317 319 320 321 322 323 324 325 327 328] [  3   5   7   9  16  25  33  42  45  46  55  57  60  63  73  77  78  82\n",
      "  84  90  94 101 110 111 119 124 137 139 140 144 145 147 163 164 168 172\n",
      " 173 175 176 177 182 210 219 222 232 237 250 256 260 262 271 274 275 277\n",
      " 288 289 296 297 301 305 309 312 314 315 318 326]\n",
      "263 66\n",
      "[  0   1   2   3   4   5   7   8   9  11  12  13  14  16  20  21  23  25\n",
      "  26  27  28  29  32  33  34  35  36  37  38  40  41  42  43  44  45  46\n",
      "  47  48  49  50  51  52  53  54  55  57  58  60  61  62  63  64  65  67\n",
      "  70  71  73  74  77  78  80  82  83  84  85  86  87  88  89  90  91  94\n",
      "  95  98  99 100 101 102 103 105 106 107 110 111 112 115 117 119 120 121\n",
      " 122 123 124 125 127 128 129 130 131 133 134 135 136 137 138 139 140 141\n",
      " 142 144 145 146 147 148 149 150 151 154 155 156 159 160 161 162 163 164\n",
      " 166 168 169 170 171 172 173 174 175 176 177 178 179 181 182 183 185 186\n",
      " 187 188 189 190 191 192 193 194 195 197 198 199 200 201 205 206 207 208\n",
      " 210 212 213 214 215 216 217 218 219 222 224 225 226 229 230 232 233 234\n",
      " 235 236 237 238 239 240 241 242 243 244 245 247 248 249 250 251 252 254\n",
      " 256 257 259 260 261 262 263 264 265 266 268 269 270 271 272 273 274 275\n",
      " 276 277 278 279 281 282 283 284 285 286 287 288 289 290 291 293 294 295\n",
      " 296 297 298 299 300 301 302 303 304 305 306 308 309 310 312 313 314 315\n",
      " 316 317 318 319 320 322 323 325 326 327 328] [  6  10  15  17  18  19  22  24  30  31  39  56  59  66  68  69  72  75\n",
      "  76  79  81  92  93  96  97 104 108 109 113 114 116 118 126 132 143 152\n",
      " 153 157 158 165 167 180 184 196 202 203 204 209 211 220 221 223 227 228\n",
      " 231 246 253 255 258 267 280 292 307 311 321 324]\n",
      "263 66\n",
      "[  0   1   3   4   5   6   7   8   9  10  11  13  14  15  16  17  18  19\n",
      "  20  21  22  24  25  26  27  30  31  32  33  34  36  39  40  41  42  43\n",
      "  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62\n",
      "  63  64  66  68  69  70  71  72  73  75  76  77  78  79  80  81  82  84\n",
      "  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104\n",
      " 105 106 108 109 110 111 113 114 116 118 119 121 122 123 124 126 128 130\n",
      " 132 134 135 136 137 138 139 140 141 143 144 145 147 149 150 151 152 153\n",
      " 156 157 158 160 161 162 163 164 165 166 167 168 169 171 172 173 174 175\n",
      " 176 177 178 180 182 184 187 188 189 190 191 192 196 200 201 202 203 204\n",
      " 205 207 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 227\n",
      " 228 230 231 232 233 235 236 237 238 240 241 242 243 245 246 247 248 249\n",
      " 250 251 252 253 255 256 257 258 259 260 262 263 264 267 268 269 270 271\n",
      " 272 273 274 275 276 277 280 281 282 283 286 288 289 290 292 293 294 295\n",
      " 296 297 299 300 301 302 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 318 319 320 321 322 323 324 325 326 327 328] [  2  12  23  28  29  35  37  38  44  65  67  74  83  85  86 107 112 115\n",
      " 117 120 125 127 129 131 133 142 146 148 154 155 159 170 179 181 183 185\n",
      " 186 193 194 195 197 198 199 206 208 224 225 226 229 234 239 244 254 261\n",
      " 265 266 278 279 284 285 287 291 298 303 304 317]\n",
      "263 66\n",
      "[  1   2   3   5   6   7   9  10  12  13  15  16  17  18  19  20  21  22\n",
      "  23  24  25  28  29  30  31  33  34  35  37  38  39  42  43  44  45  46\n",
      "  48  49  50  52  53  54  55  56  57  58  59  60  63  65  66  67  68  69\n",
      "  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88\n",
      "  90  91  92  93  94  96  97  99 101 102 104 105 106 107 108 109 110 111\n",
      " 112 113 114 115 116 117 118 119 120 121 124 125 126 127 129 130 131 132\n",
      " 133 134 137 139 140 142 143 144 145 146 147 148 149 151 152 153 154 155\n",
      " 157 158 159 160 161 163 164 165 166 167 168 169 170 172 173 174 175 176\n",
      " 177 179 180 181 182 183 184 185 186 187 188 189 190 191 193 194 195 196\n",
      " 197 198 199 201 202 203 204 205 206 207 208 209 210 211 212 214 217 219\n",
      " 220 221 222 223 224 225 226 227 228 229 231 232 234 235 236 237 239 241\n",
      " 243 244 246 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264\n",
      " 265 266 267 269 270 271 273 274 275 276 277 278 279 280 284 285 287 288\n",
      " 289 291 292 293 296 297 298 300 301 303 304 305 307 308 309 311 312 314\n",
      " 315 316 317 318 320 321 323 324 326 327 328] [  0   4   8  11  14  26  27  32  36  40  41  47  51  61  62  64  70  89\n",
      "  95  98 100 103 122 123 128 135 136 138 141 150 156 162 171 178 192 200\n",
      " 213 215 216 218 230 233 238 240 242 245 247 248 249 268 272 281 282 283\n",
      " 286 290 294 295 299 302 306 310 313 319 322 325]\n",
      "263 66\n",
      "[  0   2   3   4   5   6   7   8   9  10  11  12  14  15  16  17  18  19\n",
      "  22  23  24  25  26  27  28  29  30  31  32  33  35  36  37  38  39  40\n",
      "  41  42  44  45  46  47  51  55  56  57  59  60  61  62  63  64  65  66\n",
      "  67  68  69  70  72  73  74  75  76  77  78  79  81  82  83  84  85  86\n",
      "  89  90  92  93  94  95  96  97  98 100 101 103 104 107 108 109 110 111\n",
      " 112 113 114 115 116 117 118 119 120 122 123 124 125 126 127 128 129 131\n",
      " 132 133 135 136 137 138 139 140 141 142 143 144 145 146 147 148 150 152\n",
      " 153 154 155 156 157 158 159 162 163 164 165 167 168 170 171 172 173 175\n",
      " 176 177 178 179 180 181 182 183 184 185 186 192 193 194 195 196 197 198\n",
      " 199 200 202 203 204 206 208 209 210 211 213 215 216 218 219 220 221 222\n",
      " 223 224 225 226 227 228 229 230 231 232 233 234 237 238 239 240 242 244\n",
      " 245 246 247 248 249 250 253 254 255 256 258 260 261 262 265 266 267 268\n",
      " 271 272 274 275 277 278 279 280 281 282 283 284 285 286 287 288 289 290\n",
      " 291 292 294 295 296 297 298 299 301 302 303 304 305 306 307 309 310 311\n",
      " 312 313 314 315 317 318 319 321 322 324 325 326] [  1  13  20  21  34  43  48  49  50  52  53  54  58  71  80  87  88  91\n",
      "  99 102 105 106 121 130 134 149 151 160 161 166 169 174 187 188 189 190\n",
      " 191 201 205 207 212 214 217 235 236 241 243 251 252 257 259 263 264 269\n",
      " 270 273 276 293 300 308 316 320 323 327 328]\n",
      "264 65\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# kfold.split() returns indices to split data into training and test set\n",
    "for train_idx, test_idx in kfold.split(X_train, y_train):\n",
    "    print(train_idx, test_idx)\n",
    "    print(len(train_idx), len(test_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DLModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(DLModel, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "input_size = 20530\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [05:03<00:00,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9157\n",
      "Test AUC: 0.8764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Instantiate the model\n",
    "model = DLModel(input_size).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        X_batch = X_train[i:i+batch_size]\n",
    "        y_batch = y_train[i:i+batch_size]\n",
    "        \n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch.view(-1, 1))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# test accuracy\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test)\n",
    "    y_pred = y_pred.round()\n",
    "    acc = y_pred.eq(y_test.view_as(y_pred)).sum() / float(len(y_test))\n",
    "    print(f'Test accuracy: {acc:.4f}')\n",
    "\n",
    "# test auc\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test)\n",
    "    y_pred = y_pred.round()\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    print(f'Test AUC: {auc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a little deeper model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DLModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(DLModel, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [05:42<00:00,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8916\n",
      "Test AUC: 0.8377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Instantiate the model\n",
    "model = DLModel(input_size).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        X_batch = X_train[i:i+batch_size]\n",
    "        y_batch = y_train[i:i+batch_size]\n",
    "        \n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch.view(-1, 1))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# test accuracy\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test)\n",
    "    y_pred = y_pred.round()\n",
    "    acc = y_pred.eq(y_test.view_as(y_pred)).sum() / float(len(y_test))\n",
    "    print(f'Test accuracy: {acc:.4f}')\n",
    "\n",
    "# test auc\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test)\n",
    "    y_pred = y_pred.round()\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    print(f'Test AUC: {auc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a deeeep model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DLModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(DLModel, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [06:32<00:00,  3.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9036\n",
      "Test AUC: 0.8927\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Instantiate the model\n",
    "model = DLModel(input_size).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        X_batch = X_train[i:i+batch_size]\n",
    "        y_batch = y_train[i:i+batch_size]\n",
    "        \n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch.view(-1, 1))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# test accuracy\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test)\n",
    "    y_pred = y_pred.round()\n",
    "    acc = y_pred.eq(y_test.view_as(y_pred)).sum() / float(len(y_test))\n",
    "    print(f'Test accuracy: {acc:.4f}')\n",
    "\n",
    "# test auc\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test)\n",
    "    y_pred = y_pred.round()\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    print(f'Test AUC: {auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
