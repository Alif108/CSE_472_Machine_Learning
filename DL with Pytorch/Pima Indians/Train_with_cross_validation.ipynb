{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Cross Validation\n",
    "\n",
    "### K-Fold Cross Validation\n",
    "K-Fold Cross Validation is a common method to evaluate a model. The dataset is split into K folds. Each fold is used as the test set in turn while the rest are used as the training set. The final score is the average of the scores of all folds. The advantage of K-Fold Cross Validation is that all data are used for training and testing. The disadvantage is that it is computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>6</th>\n",
       "      <th>148</th>\n",
       "      <th>72</th>\n",
       "      <th>35</th>\n",
       "      <th>0</th>\n",
       "      <th>33.6</th>\n",
       "      <th>0.627</th>\n",
       "      <th>50</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>116</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   6  148  72  35    0  33.6  0.627  50  1\n",
       "0  1   85  66  29    0  26.6  0.351  31  0\n",
       "1  8  183  64   0    0  23.3  0.672  32  1\n",
       "2  1   89  66  23   94  28.1  0.167  21  0\n",
       "3  0  137  40  35  168  43.1  2.288  33  1\n",
       "4  5  116  74   0    0  25.6  0.201  30  0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset/pima_indians_diabetes.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((767, 8), (767,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((613, 8), (154, 8), (613,), (154,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([613, 8]),\n",
       " torch.Size([154, 8]),\n",
       " torch.Size([613]),\n",
       " torch.Size([154]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   3   4   5   7   9  10  11  12  13  14  15  16  17  19  20\n",
      "  21  22  23  24  25  27  28  30  31  32  33  36  37  38  39  40  41  42\n",
      "  44  45  46  47  48  49  50  51  52  53  54  55  58  59  61  62  63  64\n",
      "  65  66  67  68  70  71  72  73  74  75  76  77  79  80  81  82  84  85\n",
      "  86  87  88  89  90  91  93  94  95  96  97  98  99 100 101 103 104 108\n",
      " 109 110 111 112 114 115 116 117 118 119 120 121 122 123 125 127 128 129\n",
      " 130 131 132 133 135 136 137 138 141 142 144 145 147 148 149 150 152 154\n",
      " 155 157 158 160 161 162 164 165 166 167 168 169 170 171 172 173 175 176\n",
      " 178 179 180 181 182 183 185 187 188 189 191 192 193 194 195 196 197 199\n",
      " 201 203 204 205 206 207 208 209 210 211 212 213 215 216 217 218 219 220\n",
      " 221 223 224 225 226 227 228 231 232 233 234 235 236 237 238 239 241 242\n",
      " 243 244 245 248 249 250 251 252 255 256 257 259 260 261 262 263 265 267\n",
      " 268 270 272 273 275 276 277 278 279 281 282 283 284 285 286 287 288 289\n",
      " 290 293 295 297 300 302 303 304 305 306 307 308 309 310 313 314 317 318\n",
      " 319 320 322 323 324 325 326 327 328 329 330 331 333 334 335 336 338 341\n",
      " 346 347 348 349 350 351 353 354 355 356 359 360 361 362 363 364 365 366\n",
      " 367 369 370 373 375 376 377 378 379 380 381 383 385 386 387 388 389 390\n",
      " 391 392 394 395 396 397 399 400 401 402 403 404 406 407 408 409 410 412\n",
      " 413 414 415 416 417 418 422 424 425 426 427 431 432 433 434 436 437 438\n",
      " 439 440 441 442 443 445 446 447 448 450 451 453 454 455 457 459 460 461\n",
      " 462 463 464 465 468 469 470 471 472 473 474 475 476 478 479 480 481 482\n",
      " 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 499 500 501\n",
      " 502 503 504 505 506 508 509 510 511 512 513 514 516 517 518 519 520 521\n",
      " 522 523 524 526 527 528 530 532 533 534 535 536 537 538 539 540 541 542\n",
      " 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560\n",
      " 562 563 565 566 567 568 569 570 571 572 574 575 577 579 580 581 582 583\n",
      " 585 586 590 591 592 593 594 596 597 598 599 600 601 603 604 605 606 607\n",
      " 608 609 610 612] [  6   8  18  26  29  34  35  43  56  57  60  69  78  83  92 102 105 106\n",
      " 107 113 124 126 134 139 140 143 146 151 153 156 159 163 174 177 184 186\n",
      " 190 198 200 202 214 222 229 230 240 246 247 253 254 258 264 266 269 271\n",
      " 274 280 291 292 294 296 298 299 301 311 312 315 316 321 332 337 339 340\n",
      " 342 343 344 345 352 357 358 368 371 372 374 382 384 393 398 405 411 419\n",
      " 420 421 423 428 429 430 435 444 449 452 456 458 466 467 477 498 507 515\n",
      " 525 529 531 561 564 573 576 578 584 587 588 589 595 602 611]\n",
      "490 123\n",
      "[  0   1   2   3   4   5   6   8   9  10  13  17  18  19  20  21  23  24\n",
      "  25  26  28  29  30  32  34  35  38  40  42  43  45  46  47  48  53  54\n",
      "  55  56  57  58  60  61  63  64  69  70  71  73  74  75  76  77  78  79\n",
      "  80  81  83  84  85  86  87  89  90  91  92  93  94  95  97  98  99 100\n",
      " 101 102 103 104 105 106 107 108 109 110 113 114 115 116 117 118 119 120\n",
      " 122 124 125 126 127 128 129 131 132 134 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 150 151 152 153 154 155 156 158 159 160 161 163 164\n",
      " 165 167 169 171 172 173 174 175 176 177 178 179 180 181 182 183 184 186\n",
      " 187 189 190 191 192 193 194 195 198 200 201 202 203 204 205 206 207 208\n",
      " 209 210 211 212 213 214 216 217 218 219 220 221 222 223 224 225 226 227\n",
      " 228 229 230 234 236 237 238 239 240 242 243 244 246 247 248 249 250 251\n",
      " 253 254 255 256 257 258 259 261 263 264 265 266 267 268 269 270 271 272\n",
      " 273 274 275 276 277 279 280 281 283 285 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298 299 301 302 303 304 305 306 307 309 310 311 312 315\n",
      " 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 334\n",
      " 336 337 338 339 340 341 342 343 344 345 346 347 348 350 351 352 353 354\n",
      " 356 357 358 359 360 361 362 363 364 365 367 368 369 370 371 372 373 374\n",
      " 375 377 379 380 381 382 383 384 385 387 388 390 391 392 393 394 395 397\n",
      " 398 399 400 401 402 403 404 405 406 407 408 411 412 413 415 416 419 420\n",
      " 421 423 424 425 426 427 428 429 430 431 432 434 435 437 438 439 441 443\n",
      " 444 447 449 450 451 452 453 454 455 456 457 458 459 462 463 464 465 466\n",
      " 467 469 470 471 472 473 475 476 477 478 479 480 481 482 483 486 487 488\n",
      " 492 493 494 495 496 498 499 501 502 503 504 505 506 507 508 509 511 512\n",
      " 513 514 515 516 518 520 521 522 523 524 525 528 529 530 531 532 534 535\n",
      " 536 537 538 539 542 548 549 550 552 553 554 555 556 557 558 560 561 562\n",
      " 564 565 566 568 571 573 574 575 576 577 578 580 581 582 583 584 585 586\n",
      " 587 588 589 590 591 592 594 595 596 598 600 601 602 603 604 605 606 607\n",
      " 608 610 611 612] [  7  11  12  14  15  16  22  27  31  33  36  37  39  41  44  49  50  51\n",
      "  52  59  62  65  66  67  68  72  82  88  96 111 112 121 123 130 133 135\n",
      " 149 157 162 166 168 170 185 188 196 197 199 215 231 232 233 235 241 245\n",
      " 252 260 262 278 282 284 300 308 313 314 333 335 349 355 366 376 378 386\n",
      " 389 396 409 410 414 417 418 422 433 436 440 442 445 446 448 460 461 468\n",
      " 474 484 485 489 490 491 497 500 510 517 519 526 527 533 540 541 543 544\n",
      " 545 546 547 551 559 563 567 569 570 572 579 593 597 599 609]\n",
      "490 123\n",
      "[  0   1   2   3   5   6   7   8   9  11  12  13  14  15  16  17  18  19\n",
      "  20  21  22  23  24  26  27  28  29  31  32  33  34  35  36  37  38  39\n",
      "  40  41  42  43  44  45  49  50  51  52  53  56  57  58  59  60  61  62\n",
      "  63  64  65  66  67  68  69  71  72  74  75  76  77  78  80  82  83  84\n",
      "  85  86  88  89  92  94  96  97  98 102 103 104 105 106 107 109 111 112\n",
      " 113 114 117 118 119 120 121 122 123 124 126 128 130 133 134 135 136 137\n",
      " 138 139 140 142 143 144 145 146 147 148 149 151 153 154 155 156 157 158\n",
      " 159 160 161 162 163 164 165 166 167 168 169 170 171 173 174 175 176 177\n",
      " 178 179 180 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 208 209 211 212 213 214 215 216 219 222\n",
      " 223 224 225 226 227 228 229 230 231 232 233 234 235 237 238 239 240 241\n",
      " 242 244 245 246 247 249 250 252 253 254 255 256 257 258 260 261 262 263\n",
      " 264 266 267 268 269 271 273 274 275 277 278 279 280 281 282 283 284 285\n",
      " 287 288 289 290 291 292 294 295 296 297 298 299 300 301 302 303 305 307\n",
      " 308 309 311 312 313 314 315 316 317 318 321 325 326 327 328 329 331 332\n",
      " 333 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351\n",
      " 352 354 355 357 358 359 360 361 362 364 365 366 367 368 369 370 371 372\n",
      " 373 374 376 378 381 382 384 385 386 387 389 392 393 396 397 398 399 400\n",
      " 401 402 403 405 406 409 410 411 413 414 415 416 417 418 419 420 421 422\n",
      " 423 424 425 426 427 428 429 430 432 433 434 435 436 437 438 439 440 441\n",
      " 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459\n",
      " 460 461 463 464 466 467 468 469 470 471 473 474 475 476 477 478 479 481\n",
      " 483 484 485 487 489 490 491 494 495 497 498 500 507 509 510 511 512 513\n",
      " 514 515 517 518 519 520 524 525 526 527 528 529 531 532 533 535 536 537\n",
      " 538 540 541 542 543 544 545 546 547 548 549 550 551 555 556 558 559 561\n",
      " 562 563 564 566 567 568 569 570 572 573 574 575 576 577 578 579 581 584\n",
      " 585 587 588 589 591 592 593 594 595 597 598 599 600 601 602 604 605 606\n",
      " 608 609 610 611] [  4  10  25  30  46  47  48  54  55  70  73  79  81  87  90  91  93  95\n",
      "  99 100 101 108 110 115 116 125 127 129 131 132 141 150 152 172 181 182\n",
      " 206 207 210 217 218 220 221 236 243 248 251 259 265 270 272 276 286 293\n",
      " 304 306 310 319 320 322 323 324 330 334 353 356 363 375 377 379 380 383\n",
      " 388 390 391 394 395 404 407 408 412 431 462 465 472 480 482 486 488 492\n",
      " 493 496 499 501 502 503 504 505 506 508 516 521 522 523 530 534 539 552\n",
      " 553 554 557 560 565 571 580 582 583 586 590 596 603 607 612]\n",
      "490 123\n",
      "[  1   4   5   6   7   8   9  10  11  12  13  14  15  16  18  19  20  22\n",
      "  24  25  26  27  29  30  31  32  33  34  35  36  37  39  41  42  43  44\n",
      "  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  62  64\n",
      "  65  66  67  68  69  70  72  73  74  75  77  78  79  81  82  83  86  87\n",
      "  88  90  91  92  93  95  96  97  98  99 100 101 102 105 106 107 108 110\n",
      " 111 112 113 114 115 116 118 121 122 123 124 125 126 127 128 129 130 131\n",
      " 132 133 134 135 136 137 139 140 141 142 143 144 145 146 149 150 151 152\n",
      " 153 154 155 156 157 158 159 160 162 163 164 165 166 168 169 170 171 172\n",
      " 173 174 175 176 177 178 179 181 182 183 184 185 186 187 188 190 193 195\n",
      " 196 197 198 199 200 202 204 205 206 207 210 213 214 215 216 217 218 219\n",
      " 220 221 222 224 225 226 227 228 229 230 231 232 233 235 236 239 240 241\n",
      " 243 245 246 247 248 250 251 252 253 254 257 258 259 260 262 263 264 265\n",
      " 266 268 269 270 271 272 274 276 278 280 281 282 283 284 286 289 290 291\n",
      " 292 293 294 296 298 299 300 301 302 304 306 308 310 311 312 313 314 315\n",
      " 316 319 320 321 322 323 324 327 328 329 330 332 333 334 335 337 338 339\n",
      " 340 342 343 344 345 349 350 351 352 353 355 356 357 358 359 360 361 363\n",
      " 364 365 366 368 371 372 373 374 375 376 377 378 379 380 382 383 384 386\n",
      " 388 389 390 391 392 393 394 395 396 398 401 402 403 404 405 406 407 408\n",
      " 409 410 411 412 413 414 415 417 418 419 420 421 422 423 426 427 428 429\n",
      " 430 431 432 433 434 435 436 437 438 440 441 442 444 445 446 447 448 449\n",
      " 452 453 454 456 458 459 460 461 462 463 464 465 466 467 468 470 472 474\n",
      " 476 477 478 479 480 482 483 484 485 486 488 489 490 491 492 493 495 496\n",
      " 497 498 499 500 501 502 503 504 505 506 507 508 510 511 513 515 516 517\n",
      " 519 521 522 523 525 526 527 529 530 531 533 534 535 536 538 539 540 541\n",
      " 542 543 544 545 546 547 548 550 551 552 553 554 555 557 558 559 560 561\n",
      " 563 564 565 567 569 570 571 572 573 574 575 576 578 579 580 581 582 583\n",
      " 584 586 587 588 589 590 592 593 594 595 596 597 599 600 602 603 605 606\n",
      " 607 608 609 611 612] [  0   2   3  17  21  23  28  38  40  61  63  71  76  80  84  85  89  94\n",
      " 103 104 109 117 119 120 138 147 148 161 167 180 189 191 192 194 201 203\n",
      " 208 209 211 212 223 234 237 238 242 244 249 255 256 261 267 273 275 277\n",
      " 279 285 287 288 295 297 303 305 307 309 317 318 325 326 331 336 341 346\n",
      " 347 348 354 362 367 369 370 381 385 387 397 399 400 416 424 425 439 443\n",
      " 450 451 455 457 469 471 473 475 481 487 494 509 512 514 518 520 524 528\n",
      " 532 537 549 556 562 566 568 577 585 591 598 601 604 610]\n",
      "491 122\n",
      "[  0   2   3   4   6   7   8  10  11  12  14  15  16  17  18  21  22  23\n",
      "  25  26  27  28  29  30  31  33  34  35  36  37  38  39  40  41  43  44\n",
      "  46  47  48  49  50  51  52  54  55  56  57  59  60  61  62  63  65  66\n",
      "  67  68  69  70  71  72  73  76  78  79  80  81  82  83  84  85  87  88\n",
      "  89  90  91  92  93  94  95  96  99 100 101 102 103 104 105 106 107 108\n",
      " 109 110 111 112 113 115 116 117 119 120 121 123 124 125 126 127 129 130\n",
      " 131 132 133 134 135 138 139 140 141 143 146 147 148 149 150 151 152 153\n",
      " 156 157 159 161 162 163 166 167 168 170 172 174 177 180 181 182 184 185\n",
      " 186 188 189 190 191 192 194 196 197 198 199 200 201 202 203 206 207 208\n",
      " 209 210 211 212 214 215 217 218 220 221 222 223 229 230 231 232 233 234\n",
      " 235 236 237 238 240 241 242 243 244 245 246 247 248 249 251 252 253 254\n",
      " 255 256 258 259 260 261 262 264 265 266 267 269 270 271 272 273 274 275\n",
      " 276 277 278 279 280 282 284 285 286 287 288 291 292 293 294 295 296 297\n",
      " 298 299 300 301 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 318 319 320 321 322 323 324 325 326 330 331 332 333 334 335 336 337\n",
      " 339 340 341 342 343 344 345 346 347 348 349 352 353 354 355 356 357 358\n",
      " 362 363 366 367 368 369 370 371 372 374 375 376 377 378 379 380 381 382\n",
      " 383 384 385 386 387 388 389 390 391 393 394 395 396 397 398 399 400 404\n",
      " 405 407 408 409 410 411 412 414 416 417 418 419 420 421 422 423 424 425\n",
      " 428 429 430 431 433 435 436 439 440 442 443 444 445 446 448 449 450 451\n",
      " 452 455 456 457 458 460 461 462 465 466 467 468 469 471 472 473 474 475\n",
      " 477 480 481 482 484 485 486 487 488 489 490 491 492 493 494 496 497 498\n",
      " 499 500 501 502 503 504 505 506 507 508 509 510 512 514 515 516 517 518\n",
      " 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 537 539\n",
      " 540 541 543 544 545 546 547 549 551 552 553 554 556 557 559 560 561 562\n",
      " 563 564 565 566 567 568 569 570 571 572 573 576 577 578 579 580 582 583\n",
      " 584 585 586 587 588 589 590 591 593 595 596 597 598 599 601 602 603 604\n",
      " 607 609 610 611 612] [  1   5   9  13  19  20  24  32  42  45  53  58  64  74  75  77  86  97\n",
      "  98 114 118 122 128 136 137 142 144 145 154 155 158 160 164 165 169 171\n",
      " 173 175 176 178 179 183 187 193 195 204 205 213 216 219 224 225 226 227\n",
      " 228 239 250 257 263 268 281 283 289 290 302 327 328 329 338 350 351 359\n",
      " 360 361 364 365 373 392 401 402 403 406 413 415 426 427 432 434 437 438\n",
      " 441 447 453 454 459 463 464 470 476 478 479 483 495 511 513 535 536 538\n",
      " 542 548 550 555 558 574 575 581 592 594 600 605 606 608]\n",
      "491 122\n"
     ]
    }
   ],
   "source": [
    "# kfold.split() returns indices to split data into training and test set\n",
    "for train_idx, test_idx in kfold.split(X_train, y_train):\n",
    "    print(train_idx, test_idx)\n",
    "    print(len(train_idx), len(test_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimaClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, hidden_size2, output_size):\n",
    "        super(PrimaClassifier, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size2, hidden_size2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size2, output_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "input_size = 8\n",
    "hidden_size = 16\n",
    "hidden_size2 = 4\n",
    "output_size = 1\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7967\n",
      "Fold 2/5\n",
      "Validation accuracy: 0.7073\n",
      "Fold 3/5\n",
      "Validation accuracy: 0.6179\n",
      "Fold 4/5\n",
      "Validation accuracy: 0.6967\n",
      "Fold 5/5\n",
      "Validation accuracy: 0.7295\n",
      "Train accuracy: 0.8842\n",
      "Average validation accuracy: 0.7096\n",
      "Test accuracy: 0.7273\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Define the number of splits for cross-validation\n",
    "n_splits = 5\n",
    "\n",
    "# Initialize the KFold object\n",
    "kfold = KFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "# Initialize a list to store the accuracy scores for each fold\n",
    "accuracy_scores = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train, y_train)):\n",
    "    print(f\"Fold {fold+1}/{n_splits}\")\n",
    "    \n",
    "    # Get the training and validation data for this fold\n",
    "    X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "    \n",
    "    # Convert the data to PyTorch tensors\n",
    "    X_train_fold = torch.tensor(X_train_fold, dtype=torch.float32)\n",
    "    X_val_fold = torch.tensor(X_val_fold, dtype=torch.float32)\n",
    "    y_train_fold = torch.tensor(y_train_fold, dtype=torch.float32)\n",
    "    y_val_fold = torch.tensor(y_val_fold, dtype=torch.float32)\n",
    "    \n",
    "    # Move the data to the device (e.g., GPU)\n",
    "    X_train_fold = X_train_fold.to(device)\n",
    "    X_val_fold = X_val_fold.to(device)\n",
    "    y_train_fold = y_train_fold.to(device)\n",
    "    y_val_fold = y_val_fold.to(device)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = PrimaClassifier(input_size, hidden_size, hidden_size2, output_size).to(device)\n",
    "    \n",
    "    # Define the loss function and optimizer\n",
    "    loss_fn = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, len(X_train_fold), batch_size):\n",
    "            X_batch = X_train_fold[i:i+batch_size]\n",
    "            y_batch = y_train_fold[i:i+batch_size]\n",
    "            \n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch.view(-1, 1))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    with torch.no_grad():\n",
    "        y_pred_val = model(X_val_fold)\n",
    "        y_pred_val = y_pred_val.round()\n",
    "        acc_val = y_pred_val.eq(y_val_fold.view_as(y_pred_val)).sum() / float(len(y_val_fold))\n",
    "        accuracy_scores.append(acc_val.item())\n",
    "        print(f\"Validation accuracy: {acc_val:.4f}\")\n",
    "\n",
    "# train accuracy\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_train)\n",
    "    y_pred = y_pred.round()\n",
    "    acc = y_pred.eq(y_train.view_as(y_pred)).sum() / float(len(y_train))\n",
    "    print(f'Train accuracy: {acc:.4f}')\n",
    "\n",
    "# Calculate the average accuracy across all folds\n",
    "avg_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
    "print(f\"Average validation accuracy: {avg_accuracy:.4f}\")\n",
    "\n",
    "# test accuracy\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test)\n",
    "    y_pred = y_pred.round()\n",
    "    acc = y_pred.eq(y_test.view_as(y_pred)).sum() / float(len(y_test))\n",
    "    print(f'Test accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
